<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">

	<title> Stackful Coroutine Async Story in Rust </title>

	<meta name="description" content="Another rust async story">
	<meta name="author" content="Xudong-Huang">

	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<link rel="stylesheet" href="reveal.js/css/reveal.css">
	<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>

	<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
</head>

<body>

	<div class="reveal">

		<!-- Any section element inside of this container is displayed as a slide -->
		<div class="slides">
			<section>
				<h1>Stackful Coroutine in Rust</h1>
				<h3>Another async story</h3>
				<p>
					<small>Created by <a href="https://github.com/Xudong-Huang">Xudong-Huang</a> </small>
				</p>
				<aside class="notes">
					Different with the offical FUTURES which is stackless
				</aside>
			</section>

			<section data-markdown>
				<script type="text/template">
						## About me
						* My name is 黄旭东
						* eariler rust user
						* work in SDAG team (DAG base blockchain)

					</script>
			</section>

			<section data-markdown>
				<script type="text/template">
					## Agenda
					* Stackful Generator
					* Stackful Coroutine
					* Async Io
					* Sync Primitives
					* Timer Management
					NOTE: stackful coroutine is implmemented based stackful generator
				</script>
			</section>

			<section data-markdown>
				<script type="text/template">
					## Agenda - continued
					* Coroutine Cancellation
					* Deal with contentions
					* join! and select!
					* Difference with Futures
					* Conclusion
					NOTE: the gernal blocking combination logic: join! and select!
				</script>
			</section>

			<section>
				<section data-markdown>
					<script type="text/template">
						## Stackful Generator
						
						* generator is a special function with multiple return values
						```rust
						pub type Generator<'a, A, T> = Box<GeneratorImpl<'a, A, T>>;
						```
						* `send`/`yield` would switch stack context 
							```rust
							let mut g = Gn::new_scoped(|mut s| {
								s.yield_(17); //--> `yield` switch stack to caller
								              //--> `send` switch stack back from caller
								return 42;    //--> `return` finish the generator
								              //    and switch stack back to caller
							});
							assert_eq!(g.send(()), 17); // run the generator
							assert_eq!(g.send(()), 42); // run the generator again
							assert_eq!(g.is_done(), true); // the generator is done
							// g.for_each(|v| println!("{}", v); // print 17 and 42
							```
						Notes: It defines the passed parameter type A and the return type T, of course it will have multiple return points in the generator.
						The lifetime 'a is the closure liftetime that you passed in when creating the generator. Normally it's static. Let's just ignore it
						right now. The core API is the send and yield pair that would switch context back and forth.
						the context here include a small stack allocated on the heap and all registers when you leave the generator. so next time you can
						switch back and resume the last running state.
						This is a simple example of generator.
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						## Context management
						* thread local context stack and chain generators
						<div style="text-align: center;"><img src="images/context_stack_gen.png" style="background:none; border:none; box-shadow:none;"></div>
						Note: Under the cover is a thread local context stack. On top of it is always the current generator that are running. So you can easily
						find the context where to save and restore registers and generator stack.
						When you run your code in normal thread, the context must be the thread-root at the bottom which is init in the thread local construct.
						When you invoke a generator it will push the generator context on top the context stack, and when yield out, it just pop out the generator
						contuext. So you can chain generator calls in this manner.
						In the picture we show thread->generator1->generator2->generator3.
						We also show the function frames in generator3's stack. generator could call aribitory functions. and we can yield from any function frame
						point to the generator caller.
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						## stackful generator key features
						* detect current context type
						* `send` and `yield` with parameters
						* yield internal stack refs
						* always yield to caller
						Note: Some key features of stackful generator.
						1. we can detect the current running context. when you are running code in thread, the top of
						the context stack is just the thread-root which contains a null generator stack. thread is using it's own system thread.
						2. Send and Yield can take parameters. this is very usefully in corouinte implmentation.
						3. we can yield generator internal stack refs. because when you yield out the generator stack is still holding and waiting
						for another resume.
						4. we always yield to caller, just like function return to caller. This is a diffrent behavior compared with coroutine.
					</script>
				</section>
			</section>

			<section>
				<section data-markdown>
					<script type="text/template">
						## Stackful Coroutine
						* coroutine is a special thread (user space / light)
						* scheduler running on multi-thread
						* semantic blocking is not real blocking
						* call API without directly yield
						* keep the same interface as std library
						Note:
						1. coroutine is just a special user space thread. It's very light to create and destroy, very fast to resume and suspend.
						2. and it can be scheduled on multiple thread.
						3. blocking a coroutine is not a real block just like blocking a thread would never block the OS.
						4. when you write coroutine code, you never call yiled directly. they are hide in the implementation of the blocking APIs
						5. the coroutine blocking API just keep the same interface as the std lib.

					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Difference with generator
						* coroutine is a special generator
						* stackful coroutine can yield from any point directly to scheduler
						* always yield a "kernel request"
						* when resume the scheduler send back the result
						* init with Coroutine Local Storage

						Note: coroutine is a special gerator but far more beyond that. It need a scheduler to run properly.
						while generator yield to it's caller, coroutine yield directly to the scheduler. And is's always yiled a "kernel request".
						The "kernel request" means ask for the kernel to do something useful and after that resume later. e.g. when a event happned
						resume the coroutine back and process the event.
						And the scheduler will send back the kernel request's result. whether it's an Ok value or an Error.
						Last difference with generator is that courouine has local storage, but generator doesn't have one.

					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						## user and kernel model
						<div style="text-align: center;"><img src="images/coroutine_stack.png" style="background:none; border:none; box-shadow:none;"></div>
						Note: This is a picture that shows how the user and kernel model works. First the scheduler is running in the kernel sapce which is always
						in normal thread context. And the user space is the whole bunch of code that your are writing when create a coroutinie. And in coroutines
						you can call generator. But it use a special yield function to return back to the scheduler which we call it "co_yield". Note that this is
						different from the generator yiled. generator yield always return to it's caller. courtouein yield always return to scheduler.
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						## the kernel request
						* coroutine signature
						```rust
						type CoroutineImpl = Generator<'static, EventResult, EventSubscriber>;
						```
						* the kernel request
						```rust
						// APIs running after yield to scheduler
						pub trait EventSource {
							/// kernel handler of the event, got the coroutine instance back
							fn subscribe(&mut self, _c: CoroutineImpl);
							/// after yield back process
							fn yield_back(&self, cancel: &'static Cancel) {}
						}
						```
						* construct the kernel request on stack and yield out
						Note: Coroutine is just a static generator. It's return type is the kernel request called
						"EventSubscriber" which is just a thin wrapper for arbitory kernel request. And the pass in
						type is "EventReuslt". The real kernel request must implement the "EventSource" trait. which
						contains only two functions. The "subscribe" fuction will handle kernel request. usually register
						the coroutine handle to a subsystem. the "yield_back" function  will do some processing after resuming
						back. so we have a chance to deal with cancel logic here.
						The last thing about kernel request is that we construct them on stack and yield out their refs.
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						## kernel request example
						```rust
						struct Sleep { dur: Duration }
						
						impl EventSource for Sleep {
							fn subscribe(&mut self, co: CoroutineImpl) {
								// put the coroutine into the timer list
								let sleep_co = Arc::new(AtomicOption::some(co));
								get_scheduler().add_timer(self.dur, sleep_co.clone());
							}
						}

						pub fn sleep(dur: Duration) {
							let sleeper = Sleep { dur };
							yield_with(&sleeper);
						}
						```
						Note: Example of sleep API implmentation.
					</script>
				</section>

				<section data-markdown>
					<script type="text/template">
						## uniform coroutine and generator
						```rust
						go!(|| {
							let g = Gn::new_scoped(|mut s| {
								for i in 0..1000 {
									// call coroutine API in generator, yield to scheduler
									coroutine::sleep(Duration::from_secs(1));
									// yield to caller
									s.yield_(i);
								}
								1000
							});
					
							g.for_each(|v| { dbg!(v); });
						})
						.wait()
						```
						Note: Example of calling generator in coroutine context and in generator context calling the
						blocking API. This is example will print 0 to 1000 every one sencode. Note that they will yield
						to different places. Also note that we don't need stream concept here. Just use the iterator pattern
						is ok.
					</script>
				</section>
			</section>

			<section data-markdown>
				<script type="text/template">
						## system overview
						<div style="text-align: center;"><img src="images/overview.png" style="background:none; border:none; box-shadow:none;"></div>
						Note: This is system overwiew of the croutine system. In the kernel space we have thread types of scheduler.
						The user space we support those blocking APIs. Io realtead, sync primitives. timer related, coroutine management APIs,
						blocking combination APIs and so on.
					</script>
			</section>

			<section>
				<section data-markdown>
					<script type="text/template">
						## IO sub system
						* yield io kernel request
						* every io kernel request has a timeout property
						* running io related things only on io thread
						* support TCP/UDP/Unix Socket/Windows Pipe
						Note: 1. Io related APIs like socket read will yiled io kernel request.
						2. every IO kernel request has a timeout property.
						3. Those Io blocking coroutines will always schedule coroutine on the io scheduler thread.
						4. currently we support TCP/UDP/Pipe/Unix Socket.
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## the eventloop
						* uniform io event loop (epoll/kqueue/IOCP)
						* event mode: do the real io operation in user space after resume back
						* completion mode: check the io result after resume back
						* dispatch the io request on multiple io thread based on fd
						* io timeout is managed in the eventloop thread
						Note: stackful coroutine not using mio
					</script>
				</section>
			</section>

			<section>
				<section data-markdown>
					<script type="text/template">
						## sync primitive sub system
						* yield blocker kernel request
						* save the suspending coroutines within primitive internal queue
						* scheduling the coroutines on normal scheduler with work stealing
						* support Semphore/Mutex/RwLock/CondVar/SyncFlag/MPSC channel
						Note: For those sync primitive blokcing APIs we just yield bloker kernel request.
						Each sync primitive instance has it's own internal wait queue to save those suspending
						coroutines that are wait for it. 
						And those APIs will schedule the croutines in normal scheduler with work stealing.
						SyncFlag is something like Once
						
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## the blocker
						* all sync primitives are implemented based on blocker
						* the blocker could return Timeout/Canceled/Ok
						* the blocker support running in thread context

						```rust
						pub enum Parker { Coroutine(Park), Thread(ThreadPark) }
						pub enum ParkError { Canceled, Timeout }
						pub struct Blocker { parker: Parker }
						
						impl Blocker {
							pub fn park(&self, timeout: Option<Duration>) -> Result<(), ParkError> {...}
							pub fn unpark(&self) {...}
						}
						```
						Note: The blocker could return three states.
						the primitives can be cancelled with correct behavior.
						also the blocker support running in thread context. which means you can wait a mutex in
						a thread and release the mutex signal in a courontine.
					</script>
				</section>
			</section>

			<section>
				<section data-markdown>
					<script type="text/template">
						## timer management
						* running timer manager in a separate thread
						* the timer request push to MPSC "priority queue"
						* blocker/sleep would push timeout request to the priority queue
						* when a timeout happened, throw the coroutine into global ready list
						Note: the priority queue is semi atomic. keep the interval mpsc list in binary heap.
						the same interval request will push to the same interval list. so we can quickly peek
						out which one is timed out first.
					</script>
				</section>
			</section>

			<section>
				<section data-markdown>
					<script type="text/template">
						## coroutine cancellation
						* we trigger a special panic when detect cancel
						* the cancel panic would be caught and we have chance to mark the coroutine as cancelled
						* can only detect cancel when switch from/to kernel
						* the cancel panic would trigger stack unwind and may has bad side effect
						```rust
						let j = go!(move || {
							coroutine::sleep(Duration::from_secs(1000000);
						});
						unsafe { j.coroutine().cancel() };
						assert!(j.joint().is_err()); // Err(Cancel)
						```
						Note: during running we have no way to stop the code in user space. This is different from real OS.
					</script>
				</section>
			</section>

			<section>
				<section data-markdown>
					<script type="text/template">
						## Deal with contentions
						* user space and kernel space contention
							- sub system may run the coroutine while it's still in kernel processing
						* sub system contentions
							- io and cancel
							- blocker timeout
							- blocker cancel
						* sync primitive contentions
						Note: Once when a coroutine handle is registered to a subsystem, it may run immediatly
						on other thread. So we need to wait it's kernel processing done before we drop the
						kenel request on the stack.
						Cancel and timeout are other async events that will competing with the normal logic.
						Cancel io on windows need call the CancelIo system API.
						Timeout is another aysnc event. when it happened we must correctly process the sync primitive
						logic.
						Of course we have sync primitive contentions. processing them right is hard and tricky, involve
						a lot of atomic operations.
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## the solutions
						* user space and kernel space contention
							- Delay drop kernel request until the kernel `subscribe()` done
						* use lockless data structures
							- AtomicOption - only one sub system can grab the coroutine handle
							- atomic queues - Mutex/Semphore queue
						* atomic event exchange algorithm
							- make sure event processed once and only once between event consumer and producer
						Note: use atoimc data structure from crossbeam.
					</script>
				</section>
				<section data-markdown>
					<script type="text/template">
						## Atomic event procecess
						* the event consumer loop in a thread

						```rust
						// try to consume the coroutine handle
						while let Some(io_data) = event_selector.poll() {
							if let Some(co) = io_data.co.take(Ordering::Acquire) {
								// process the event in kernel
								......
								co.resume();
							}}						
						```
						* the event producer in another thread

						```rust
						// register the coroutine handle to io sub system
						io_data.co.swap(co, Ordering::Release);
						// try to run the event process logic
						if let Some(co) = io_data.co.take(Ordering::Acquire) {
							// process the event in kernel
							......
							return co.resume(); }
						```
						Note: explain the algorithm
					</script>
				</section>
			</section>

			<section>
				<section data-markdown>
					<script type="text/template">
						## uniform thread and coroutine
						* we can detect the context when calling blocking API
							 - if current context is thread root
							 - if current context has coroutine local storage
						* when we are calling blocking API in thread context it has the same effect as calling corresponding std API
						Note: blocking APIs are context awaring.
					</script>
				</section>
			</section>

			<section>
				<section data-markdown>
					<script type="text/template">
						## combine blocking logic
						* `join!()` macro wait all things done
						```rust
						// ref scoped coroutines
						join!(
							rx1.recv().unwrap(),
							mutex1.lock().unwrap(),
						);
						```
						* `select!()` macro wait any thing done first and cancel others
						```rust
						// ref cqueue APIs
						let n = select!(
							x = rx1.recv().unwrap() => println!("got x={} first", x),
							y = rx2.recv().unwrap() => println!("got y={} first", y),
							_ = sleep(Duration::from_secs(1)) => println!("timeout!"),
						);
						```
						Note: the select example shows if the two channle can receive a value within one second.
					</script>
				</section>
			</section>

			<section>
				<section data-markdown>
					<script type="text/template">
						## difference with futures
						* The Good
						    - intuitive to learn and use
							- no need to mark with `async` and `await`
							- the APIs could be run within thread context
							- the APIs are compatible with std and stable
						* The Bad
						    - consume more memory
							- context switching is not free
							- more easily stack overflow
						Note: you should tune the stack usage of the application
						      there are tools to show the stack usage
					</script>
				</section>
			</section>

			<section
				data-background-iframe="https://www.techempower.com/benchmarks/#section=data-r17&hw=cl&test=plaintext">
				<div
					style="position: absolute; width: 40%; right: 0; box-shadow: 0 1px 4px rgba(0,0,0,0.5), 0 5px 25px rgba(0,0,0,0.2); background-color: rgba(0, 0, 0, 0.9); color: #fff; padding: 20px; font-size: 20px; text-align: left;">
					<p> CONCLUSION </p>
					<p> - uniform generator and coroutine </p>
					<p> - uniform thread and coroutine </p>
					<p> - uniform linux/bsd/windows </p>
					<p> - compatible API with std library </p>
					<p> - intuitive usage and high performance </p>
				</div>
			</section>

			<section style="text-align: center;">
				<h1>Thanks</h1>
				<p style="text-align: left;">
					- <a href="https://https://github.com/Xudong-Huang/generator-rs">generator-rs</a> <br>
					- <a href="https://github.com/Xudong-Huang/May">May</a>
				</p>
			</section>

		</div>

	</div>

	<script src="reveal.js/lib/js/head.min.js"></script>
	<script src="reveal.js/js/reveal.js"></script>

	<script>

		// More info https://github.com/hakimel/reveal.js#configuration
		Reveal.initialize({
			controls: true,
			progress: true,
			history: true,
			center: true,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// More info https://github.com/hakimel/reveal.js#dependencies
			dependencies: [
				{ src: 'reveal.js/lib/js/classList.js', condition: function () { return !document.body.classList; } },
				{ src: 'reveal.js/plugin/markdown/marked.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				{ src: 'reveal.js/plugin/markdown/markdown.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				{ src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function () { hljs.initHighlightingOnLoad(); } },
				{ src: 'reveal.js/plugin/search/search.js', async: true },
				{ src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
				{ src: 'reveal.js/plugin/notes/notes.js', async: true }
			]
		});

	</script>

</body>

</html>